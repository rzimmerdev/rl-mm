\documentclass{article}
\usepackage{epsfig}
\usepackage{hyperref}
\renewcommand{\baselinestretch}{1}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.5in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\parindent}{.3in}
\begin{document}

\leftline{Pat Q.~Student}
\leftline{AME 20231}
\leftline{18 January 2024}

\begin{section}

Market making in financial markets is a trading approach that involves continuously quoting buy and sell prices. Market makers are essential, as they help narrowing bid-ask spreads, reduce price volatility, and maintain market stability, particularly in times of uncertainty by trading and providing liquidity in the market. Dealing with supply and demand imbalances, as well as the risks of holding inventory and speculation is an intrinsic aspect of market making and a focus of recent research. Correctly reducing inventory risk benefits not only individual market makers but all market participants as a whole. With the increase of computational power and data-driven automated systems, placing optimal bid and ask prices as a market maker is becoming an almost completely automatized task, but such a transition comes with additional caveats, such as slippage, overnight inventory and adverse market movements risks on agent inventory. 

Recent research focusing on Reinforcement Learning (RL) shows the technique to be a promising approach for optimizing placed bid-ask prices by market making strategies. The RL framework defines market makers as agents that learn by interacting with an environment - which in our case is the limit order book - and optimizing their decision-making policies through trial and error and the respective outcome (also called reward) of the environment. The primary objective of RL-based market making is to find the optimal policy that maximizes cumulative rewards, which translates to choosing prices that provide positive spread differences while minimizing risk factors such as agent inventory or price volatility. The RL approach is based on the Bellman equation for state values or state-action pair values. These equations recursively defines the value of a policy by considering the expected return of both immediate or discounted future rewards. A common implementation of the RL approach is to continuously update the agent's policy based on the sampled environment information and rewards. This results in the agents being able to dynamically change their strategies and adapt to evolving market conditions. 

Unrestricted RL-based market making, however, can lead to potentially risky behaviors. In financial markets, and in the Market Making scenario inventory risk and market impact are not to be ignored, as agents accumulating large inventory positions become exposed to significant price fluctuations and adverse market direction changes. This makes holding inventory result in an unwanted position exposing the agent to substantial losses and risks. Therefore, recent literature discusses the use of restrictions in the RL framework to introduce agent restrictionso that are important in ensuring stability and practicality for real-world implementations. 

⁤In the context of this paper we will discuss restrictions in two possible forms: as hard equality or interval limits, or as incentives integrated into the agent's reward structure. ⁤⁤Hard limits enforce strict numerical conditions, such as having zero inventory at market close, must be met. ⁤⁤On the other hand, incentive-based restrictions simply increase the loss on rewards the closer the agent is to the restriction, such as a negative reward the larger the inventory on market close, thus making the agent adverse to so called overnight risk, which will be discussed in further sections. Such restrictions are aimed to reduce target volatility, and increase possibilities of introducing domain knowledge and known stylized facts of the market into the learning process. ⁤⁤Our approach will also introduce a pragmatic and replicable way of implementing restrictions on a RL framework, contributing to reducing any form of undesired risk taken by the agent while still enhancing the computational feasibility of the RL implementation by leaning towards to more interpretable models. ⁤

Our main contributions for this paper are: a reusable design and implementation on restrictions for RL market making agents; analyze the impact on performance of RL agents adverse to overnight risk; Discuss and explore how varying degrees of restrictions, either in the form of penalty structures or hard inventory limits, affect the robustness of risk-adverse strategies under realistic market conditions.

\end{section}

\end{document}
