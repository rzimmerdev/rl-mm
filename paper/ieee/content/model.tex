\section{Implementation and Model Description}
\label{sec:implementation-and-model-description}

As previously stated in \autoref{sec:methodology}, the chosen model follows the Actor-Critic architecture,
with the Actor network learning the policy and the Critic network learning the value function.
For the actor network input, we separate the state space into two tensors: the market features and the LOB data.
The market features contain general high-level information, including the microprice, 10, 15 and 30-period moving averages,
agent inventory, and the Relative Strength Index (RSI) and Order Imbalance (OI) indicators.
The LOB data contains the ordered $N$th best bid-ask price and volume pairs.

We use a sequence of self-attention layers to capture the spatial dependencies between the different levels of the LOB,
while the market features are passed through a dense layer and concatenated with the output of the self-attention layers,
before being passed through the final dense layers as shown in \autoref{fig:actor-architecture}.
We implement the Critic network as a simple feed-forward neural network with two hidden layers, of 128 and 64 units, respectively,
with the same input tensors as the Actor network.

\begin{figure}
    \centering
    \includegraphics[width=.9\columnwidth]{images/policy}
    \caption{Actor Network Architecture}
    \label{fig:actor-architecture}
\end{figure}

To train our policy and value networks, we use the aforementioned PPO algorithm\cite{Schulman2017},
which is a model-free, on-policy approach to optimize the policy directly, as discussed in~\hyperref[subsubsec:gpi]{Subsection~\ref{subsubsec:gpi}}.
Our training loop shown in \hyperref[alg:algorithm]{Algorithm~\ref{alg:algorithm}} consists of collecting trajectories,
computing the Generalized Advantage Estimation (GAE), which is used instead of the usual returns $G_t$.
The chosen approach can be classified as \textbf{online reinforcement learning}, where the agent learns from interactions with the environment,
without the need for a pre-existing dataset, as opposed to offline reinforcement learning, where the agent learns from a static dataset.
Additionally, we use a replay buffer to store the trajectories and sample mini-batches for training the policy and value networks.
Since the replay buffer is discarded immediately after a small fixed number of epochs, the approach is closer to \textbf{on-policy learning},
as the data collected by the policy is the same as the data used for gradient updates.

\begin{algorithm}
    \begin{algorithmic}[1]
        \Require Environment, PPO model, optimizer, number of episodes $num\_episodes$
        \For{each episode in range $num\_episodes$}
            \State Reset environment and observe initial state $s$
            \For{each timestep until episode ends}
                \State Select action $a \sim \pi_{\theta}(s)$
                \State Observe reward $R_t$ and next state $s'$
                \State Store transition $(s, a, r)$ in the trajectory buffer
                \State Set $s \leftarrow s'$
            \EndFor
            \State \textbf{Compute GAE and Returns} // Policy evaluation
            \State \textbf{Update parameters} $\boldsymbol{\theta}$ \textbf{and} $\boldsymbol{\phi}$ // Policy improvement
        \EndFor
    \end{algorithmic}
    \caption{Training Loop}
    \label{alg:algorithm}
\end{algorithm}
