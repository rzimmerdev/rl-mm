\section{Introduction}
\label{sec:introduction}

Market making in financial markets involves continuously quoting buy and sell prices and dealing with supply and demand imbalances.
Market makers are essential, as they narrow the bid-ask spreads, reduce price volatility, and maintain market stability,
particularly in times of uncertainty by providing liquidity, which helps to maintain a fair and efficient market~\cite{Glosten1985, OHara1995}.
With the advent of electronic trading, placing optimal bid and ask prices as a market maker is becoming an almost completely automatized task,
but such a transition comes with the hardships of dealing with additional caveats, including slippage, market impact, adverse market regimes~\cite{Cont2010, Bouchaud2018}
and non-stationary conditions~\cite{Gasperov2021}.

The Reinforcement Learning (RL) paradigm has shown promising results for optimizing market making strategies,
where agents learn to adapt their quote placing policies through trial and error given a numerical outcome reward score\footnote{Not to be confused with financial returns.}.
The RL approach is based on the Bellman equation for state values or state-action pair values,
which recursively define the state-value of a policy as the expected return of discounted future rewards.
State-of-the-art RL algorithms, such as Proximal Policy Optimization (PPO) and Deep Q-Learning (DQN),
solve the Bellman equation by approximating the optimal policy using neural networks to learn either the policy, the value function~\cite{Sutton2018},
or both~\cite{Schulman2015, Mnih2015}, aiming to allow the agent to its quoted prices based adaptively on the observed market conditions~\cite{He2023, Bakshaev2020}.

Using historical data to train RL agents is a common practice in the literature, but it has some limitations,
as it is computationally expensive and requires a large amount of data,
besides not including the effects of market impact and inventory risk~\cite{Frey2023, Ganesh2019} during the training process.
An additional approach to creating realistic environments are agent-based simulations,
where generative agents are trained against observed market messages and used to simulate the limit order book environment~\cite{Frey2023, Ganesh2019},
but this approach has the disadvantage of trading off control over the market dynamics for replicating observed market flow,
as well as limiting the agent's adaptability to unseen market regimes, which can lead to underfitting~\cite{Jerome2022, Selser2021}
and suboptimal decision-making under scenarios where market impact and slippage have a more prominent effect on the agent's performance.
On the other hand, using stochastic models to simulate the limit order book environment is computationally cheaper and faster to train~\cite{Gasperov2021, Sun2022},
but might not generalize well to unseen market conditions.

In the context of this paper, we implement a RL-based market making agent and test its robustness and generalization capabilities under non-stationary environments.
The agent is trained on a crafted simulator that models the dynamics of a limit order book (LOB) according to a set of parameterizable stochastic processes.
This approach aims to demonstrate the effects of non-stationary environments and adverse market conditions on
a RL-based market making agent, and how the agent's decision-making policies are affected by market impact and inventory risk.
Our main contribution is implementing multiple non-stationary dynamics into a single limit order book simulator,
and how using fine-controlled non-stationary environments can enhance the agent's performance under adverse market conditions
and provide a more realistic training environment for RL agents in market-making scenarios.
We also perform a comparative analysis of the agent's performance under changing market conditions during the
trading day and benchmark it against a closed-expression optimal solution (under a simplified market model),
aiming to validate the usage of RL-based market making agents in markets with more complex dynamics.

