\section{Conclusion}
\label{sec:conclusion}
We have presented the design and implementation of a reinforcement learning agent aiming to show
the effects of adverse market conditions and non-stationary environments on control agents for market-making.
Our approach leverages online reinforcement learning by means of a simulator that models the dynamics of a limit order book (LOB)
according to a set of parameterizable stochastic processes configured to mimic observed stylized facts in real markets.
The resulting model is a continuous-time Markov Decision Process (MDP) that captures the observed stylized fact of clustered order arrival times.

The main objective of this work to validate the use of complex market-making environments as
pretraining environments for reinforcement learning agents, and to show that agents can perform well under adverse market conditions.
We aimed to show that the use of non-stationary environments and fine-controlled dynamics can be used to model known stylized facts of real markets,
and conducted experiments to validate the expected performance of the agents under these conditions.

Prospective future work on existing market model structures includes the extension of market dynamics within
the training of world models, and the implementation of hybrid models that combine both model-based and model-free approaches
to leverage the capability of RL agents to learn from non-stationary environments and adapt to known stylized facts of real markets.
