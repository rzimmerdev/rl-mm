\IEEEoverridecommandlockouts
\title{
    \LARGE
    \bfseries
    Reinforcement Learning-Based Market Making as a Stochastic Control on Non-Stationary Dynamics
    \thanks{This work was supported by the Foundation for Research of the State of São Paulo (FAPESP) under grant 2023/16028-3.}
}

\author{
    \IEEEauthorblockN{\href{https://orcid.org/0009-0008-6064-9895}{Rafael Zimmer \includegraphics[scale=0.06]{images/orcid}} and \href{https://orcid.org/0000-0001-5989-7287}{Oswaldo Luiz Do Valle Costa \includegraphics[scale=0.06]{images/orcid}}, Senior Member, IEEE}
    \thanks{Rafael Zimmer is with the Institute of Mathematics and Computer Sciences, University of São Paulo, São Paulo, Brazil \href{mailto:rafael.zimmer@usp.br}{\tt\small rafael.zimmer@usp.br}}
    \thanks{Oswaldo Luiz Do Valle Costa is with the Polytechnic School of the University of São Paulo, São Paulo, Brazil \href{mailto:mailto:oswaldo.costa@usp.br}{\tt\small oswaldo.costa@usp.br}}
}

\maketitle

\begin{abstract}
    Reinforcement Learning has emerged as a promising framework for developing adaptive and data-driven strategies,
    enabling market makers to optimize decision-making policies based on interactions with the limit order book environment.
    Historically, market-making strategies have been based on closed-form solutions or heuristic rules,
    or trained on historical data, which may not capture the full complexity of changing market dynamics.
    This paper explores the integration of a reinforcement learning agent in a market-making context,
    where the underlying market dynamics have been explicitly modeled together to capture observed stylized facts of real markets,
    including clustered order arrival times, non-stationary spreads and return drifts, stochastic order quantities and price volatility.
    These mechanisms aim to enhance stability of the resulting control agent,
    and serve to provide a more realistic training environment for reinforcement learning agents in market-making scenarios.

    Our contributions include a simulator-based environment with fully parameterizable independently distributed non-stationary dynamics,
    aimed to model the effects of adverse market conditions and common stylized facts observed in real markets.
    We also implement a reinforcement learning agent based on the Proximal-Policy Optimization (PPO) algorithm and a
    comparative evaluation of the agent's performance under varying market conditions, including varying spread regimes and return drifts.
    We compare the metrics to a closed-form optimal solution under a simplified market model,
    and our results suggest that policy gradient-based agents can effectively be used under non-stationary market conditions,
    and show that the proposed simulated environment can serve as a valuable tool for training and
    pre-training reinforcement learning agents in market-making scenarios.
\end{abstract}

\begin{IEEEkeywords}
    Reinforcement Learning, Market Making, Limit Order Book, Stochastic Control, Market Microstructure
\end{IEEEkeywords}
