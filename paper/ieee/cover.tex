%\author{
%    \IEEEauthorblockN{\href{https://orcid.org/0009-0008-6064-9895}{1\textsuperscript{st} Rafael Zimmer \includegraphics[scale=0.06]{images/orcid}}}
%    \IEEEauthorblockA{
%        \textit{Institute of Mathematics and Computer Sciences} \\
%        São Paulo, Brazil \\
%        \href{mailto:rafael.zimmer@usp.br}{rafael.zimmer@usp.br}
%    }
%    \and
%    \IEEEauthorblockN{\href{https://orcid.org/0000-0001-5989-7287}{2\textsuperscript{nd} Oswaldo Luiz Do Valle Costa \includegraphics[scale=0.06]{images/orcid}}}
%    \IEEEauthorblockA{
%        \textit{Polytechnic School of the University of São Paulo} \\
%        São Paulo, Brazil \\
%        \href{mailto:oswaldo.costa@usp.br}{oswaldo.costa@usp.br}
%    }
%}
\title{Reinforcement Learning-Based Market Making as a Stochastic Control on Non-Stationary Dynamics}

\author{
    \IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
    \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
    \   textit{name of organization (of Aff.)}\\
    City, Country \\
    email address or ORCID
    }
}

\date{\today}

\maketitle

\begin{abstract}
    Reinforcement Learning has emerged as a promising framework for developing adaptive and data-driven strategies,
    enabling market makers to optimize decision-making policies based on interactions with the limit order book environment.
    Market-making strategies can be expressed as closed-form solutions or sets of heuristic rules,
    and state-of-the-art approaches use machine learning techniques to train agents on historical data or against adversarial market agents,
    but these approaches often rely on static environments or simplified market models, and may not capture the full complexity of changing market dynamics.
    This paper explores the integration of a reinforcement learning agent in a market-making context,
    where the underlying market dynamics have been explicitly modeled as stochastic processes and combined to capture observed stylized facts of real markets,
    including clustered order arrival times, non-stationary spreads and return drifts, stochastic order quantities and price volatility.

    Our contributions include a limit order book simulator with fully parameterizable and independently distributed dynamics,
    combined to model non-stationary market conditions and adverse market regimes.
    We compare the performance of a custom reinforcement learning agent based on the Proximal-Policy Optimization (PPO) algorithm
    against additional benchmarks under our proposed simulated environment.
    The results obtained suggest complex stochastic environments can serve as a valuable tool for training RL agents under markets with non-stationary regimes.
\end{abstract}

\begin{IEEEkeywords}
    Reinforcement Learning, Market Making, Limit Order Book, Stochastic Control, Market Microstructure
\end{IEEEkeywords}
