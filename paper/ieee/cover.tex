\IEEEoverridecommandlockouts
\title{
    \LARGE
    \bfseries
    Reinforcement Learning-Based Market Making as a Stochastic Control on Non-Stationary Dynamics
    \thanks{This work was supported by the Foundation for Research of the State of São Paulo (FAPESP) under grant 2023/16028-3.}
}

\author{
    \IEEEauthorblockN{\href{https://orcid.org/0009-0008-6064-9895}{Rafael Zimmer \includegraphics[scale=0.06]{images/orcid}} and \href{https://orcid.org/0000-0001-5989-7287}{Oswaldo Luiz Do Valle Costa \includegraphics[scale=0.06]{images/orcid}}, Senior Member, IEEE}
    \thanks{Rafael Zimmer is with the Institute of Mathematics and Computer Sciences, University of São Paulo, São Paulo, Brazil \href{mailto:rafael.zimmer@usp.br}{\tt\small rafael.zimmer@usp.br}}
    \thanks{Oswaldo Luiz Do Valle Costa is with the Polytechnic School of the University of São Paulo, São Paulo, Brazil \href{mailto:mailto:oswaldo.costa@usp.br}{\tt\small oswaldo.costa@usp.br}}
}

\maketitle

\begin{abstract}
    Reinforcement Learning has emerged as a promising framework for developing adaptive and data-driven strategies,
    enabling market makers to optimize decision-making policies based on interactions with the limit order book environment.
    Market-making strategies can be expressed as closed-form solutions or sets of heuristic rules,
    and state-of-the-art approaches use machine learning techniques to train agents on historical data or against adversarial market agents,
    but these approaches often rely on static environments or simplified market models, and may not capture the full complexity of changing market dynamics.
    This paper explores the integration of a reinforcement learning agent in a market-making context,
    where the underlying market dynamics have been explicitly modeled as stochastic processes and combined to capture observed stylized facts of real markets,
    including clustered order arrival times, non-stationary spreads and return drifts, stochastic order quantities and price volatility.

    Our contributions include a limit order book simulator with fully parameterizable and independently distributed dynamics,
    combined to model non-stationary market conditions and adverse market regimes.
    We compare the performance of a custom reinforcement learning agent based on the Proximal-Policy Optimization (PPO) algorithm
    against additional benchmarks under our proposed simulated environment.
    The results obtained suggest complex stochastic environments can serve as a valuable tool for training RL agents under markets with non-stationary regimes.
\end{abstract}

\begin{IEEEkeywords}
    Reinforcement Learning, Market Making, Limit Order Book, Stochastic Control, Market Microstructure
\end{IEEEkeywords}
