% Encoding: UTF-8
@Article{Gerald1995,
  author     = {Tesauro, Gerald},
  journal    = {Commun. ACM},
  title      = {Temporal Difference Learning and TD-Gammon},
  year       = {1995},
  issn       = {0001-0782},
  month      = {mar},
  number     = {3},
  pages      = {58–68},
  volume     = {38},
  abstract   = {Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.},
  address    = {New York, NY, USA},
  doi        = {10.1145/203330.203343},
  issue_date = {March 1995},
  numpages   = {11},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/203330.203343},
}

@Article{Kaelbling1996,
  author        = {L. P. Kaelbling and M. L. Littman and A. W. Moore},
  title         = {Reinforcement Learning: A Survey},
  year          = {1996},
  abstract      = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
  archiveprefix = {arXiv},
  eprint        = {cs/9605103},
  primaryclass  = {cs.AI},
}

@Book{Sutton2018,
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  publisher = {The MIT Press},
  title     = {Reinforcement Learning: An Introduction},
  year      = {2018},
  isbn      = {978-0262039246},
  abstract  = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.

Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  url       = {http://incompleteideas.net/book/RLbook2020.pdf},
}

@Article{Gasperov2021,
  author    = {Bruno Ga{\v{s}}perov and Stjepan Begu{\v{s}}i{\'{c}} and Petra Posedel {\v{S}}imovi{\'{c}} and Zvonko Kostanj{\v{c}}ar},
  journal   = {Mathematics},
  title     = {Reinforcement Learning Approaches to Optimal Market Making},
  year      = {2021},
  month     = {oct},
  number    = {21},
  pages     = {2689},
  volume    = {9},
  abstract  = {Market making is the process whereby a market participant, called a market maker, simultaneously and repeatedly posts limit orders on both sides of the limit order book of a security in order to both provide liquidity and generate profit. Optimal market making entails dynamic adjustment of bid and ask prices in response to the market maker’s current inventory level and market conditions with the goal of maximizing a risk-adjusted return measure. This problem is naturally framed as a Markov decision process, a discrete-time stochastic (inventory) control process. Reinforcement learning, a class of techniques based on learning from observations and used for solving Markov decision processes, lends itself particularly well to it. Recent years have seen a very strong uptick in the popularity of such techniques in the field, fueled in part by a series of successes of deep reinforcement learning in other domains. The primary goal of this paper is to provide a comprehensive and up-to-date overview of the current state-of-the-art applications of (deep) reinforcement learning focused on optimal market making. The analysis indicated that reinforcement learning techniques provide superior performance in terms of the risk-adjusted return over more standard market making strategies, typically derived from analytical models.},
  doi       = {10.3390/math9212689},
  publisher = {{MDPI} {AG}},
}

@Misc{Ganesh2019,
  author        = {Sumitra Ganesh and Nelson Vadori and Mengda Xu and Hua Zheng and Prashant Reddy and Manuela Veloso},
  month         = nov,
  title         = {Reinforcement Learning for Market Making in a Multi-agent Dealer Market},
  year          = {2019},
  abstract      = {Market makers play an important role in providing liquidity to markets by continuously quoting prices at which they are willing to buy and sell, and managing inventory risk. In this paper, we build a multi-agent simulation of a dealer market and demonstrate that it can be used to understand the behavior of a reinforcement learning (RL) based market maker agent. We use the simulator to train an RL-based market maker agent with different competitive scenarios, reward formulations and market price trends (drifts). We show that the reinforcement learning agent is able to learn about its competitor's pricing policy; it also learns to manage inventory by smartly selecting asymmetric prices on the buy and sell sides (skewing), and maintaining a positive (or negative) inventory depending on whether the market price drift is positive (or negative). Finally, we propose and test reward formulations for creating risk averse RL-based market maker agents.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/arXiv.1911.05892},
  eprint        = {1911.05892},
  file          = {:Ganesh2019 - Reinforcement Learning for Market Making in a Multi Agent Dealer Market.pdf:PDF},
  keywords      = {Trading and Market Microstructure (q-fin.TR), Machine Learning (cs.LG), Multiagent Systems (cs.MA), FOS: Economics and business, FOS: Computer and information sciences},
  primaryclass  = {q-fin.TR},
  publisher     = {arXiv},
  url           = {https://arxiv.org/pdf/1911.05892.pdf},
}

@Misc{Gueant2017,
  author        = {Olivier Guéant},
  month         = may,
  title         = {Optimal market making},
  year          = {2017},
  abstract      = {Market makers provide liquidity to other market participants: they propose prices at which they stand ready to buy and sell a wide variety of assets. They face a complex optimization problem with both static and dynamic components. They need indeed to propose bid and offer/ask prices in an optimal way for making money out of the difference between these two prices (their bid-ask spread). Since they seldom buy and sell simultaneously, and therefore hold long and/or short inventories, they also need to mitigate the risk associated with price changes, and subsequently skew their quotes dynamically. In this paper, (i) we propose a general modeling framework which generalizes (and reconciles) the various modeling approaches proposed in the literature since the publication of the seminal paper "High-frequency trading in a limit order book" by Avellaneda and Stoikov, (ii) we prove new general results on the existence and the characterization of optimal market making strategies, (iii) we obtain new closed-form approximations for the optimal quotes, (iv) we extend the modeling framework to the case of multi-asset market making and we obtain general closed-form approximations for the optimal quotes of a multi-asset market maker, and (v) we show how the model can be used in practice in the specific (and original) case of two credit indices.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1605.01862},
  eprint        = {1605.01862},
  file          = {:guéant2017optimal - Optimal Market Making.pdf:PDF},
  keywords      = {Trading and Market Microstructure (q-fin.TR), FOS: Economics and business},
  primaryclass  = {q-fin.TR},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/1605.01862},
}

@Article{Avellaneda2008,
  author    = {Marco Avellaneda and Sasha Stoikov},
  journal   = {Quantitative Finance},
  title     = {High-frequency trading in a limit order book},
  year      = {2008},
  month     = {apr},
  number    = {3},
  pages     = {217--224},
  volume    = {8},
  abstract  = {We study a stock dealer’s strategy for submitting bid and ask quotes in a
limit order book. The agent faces an inventory risk due to the diffusive nature of
the stock’s mid-price and a transactions risk due to a Poisson arrival of market
buy and sell orders. After setting up the agent’s problem in a maximal expected
utility framework, we derive the solution in a two step procedure. First, the
dealer computes a personal indifference valuation for the stock, given his current
inventory. Second, he calibrates his bid and ask quotes to the market’s limit
order book. We compare this ”inventory-based” strategy to a ”naive” strategy
that is symmetric around the mid-price, by simulating stock price paths and
displaying the P&L profiles of both strategies. We find that our strategy yields
P&L profiles and final inventories that have significantly less variance than the
benchmark strategy.},
  doi       = {10.1080/14697680701381228},
  file      = {:Avellaneda2008 - High Frequency Trading in a Limit Order Book.pdf:PDF},
  publisher = {Informa {UK} Limited},
  url       = {https://math.nyu.edu/~avellane/HighFrequencyTrading.pdf},
}

@Article{selser2021optimal,
  author        = {Matias Selser and Javier Kreiner and Manuel Maurette},
  title         = {Optimal Market Making by Reinforcement Learning},
  year          = {2021},
  archiveprefix = {arXiv},
  doi           = {10.48550/arXiv.2104.04036},
  eprint        = {2104.04036},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/pdf/2104.04036.pdf},
}

@Article{Gu_ant_2012,
  author    = {Olivier Gu{\'{e}}ant and Charles-Albert Lehalle and Joaquin Fernandez-Tapia},
  title     = {Dealing with the inventory risk: a solution to the market making problem},
  journal   = {Mathematics and Financial Economics},
  year      = {2012},
  volume    = {7},
  number    = {4},
  pages     = {477--507},
  month     = {sep},
  doi       = {10.1007/s11579-012-0087-0},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007%2Fs11579-012-0087-0},
}

@Article{bakshaev2020marketmaking,
  author        = {Alexey Bakshaev},
  title         = {Market-making with reinforcement-learning (SAC)},
  journal       = {Quantitative Finance},
  year          = {2020},
  archiveprefix = {arXiv},
  doi           = {10.48550/arXiv.2008.12275},
  eprint        = {2008.12275},
  primaryclass  = {q-fin.PR},
  url           = {https://arxiv.org/pdf/2008.12275.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}
