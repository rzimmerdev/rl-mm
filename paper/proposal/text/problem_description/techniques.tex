O objetivo principal do agente de \textit{MM} é decidir dentro do intervalo de preços possíveis para o ativo o que proporcione um maior retorno financeiro para o menor risco associado, de acordo com a fronteira eficiente de execução ótima \citep{almgren2000}. O agente também pode decidir a quantidade $Q$ de ações ofertadas por determinado preço $p$, mas não tem controle direto sobre as quantidades efetivamente negociadas. Ou seja, a quantidade executada $N \leq Q$ é uma variável aleatória, dado que não é possível determinar de antemão quanto o agente agressor irá negociar com o agente de \textit{market-making} \citep{rao2020stochastic}.

Como mencionado, é possível representar essa dinâmica na forma de um problema de otimização estocástica com restrições, onde o objetivo do agente (ou controle) consiste em obter uma aproximação ótima da equação de valores de estados de Bellman $V^*(s) = \max_\pi V^{\pi}(s).$. De modo a obter a solução para a equação, definimos o processo de decisão de Markov na seção anterior $(\mathcal{S}, \mathcal{A}, T_{a}, R_{a})$. 

Há várias abordagens, algumas mencionadas anteriormente como métodos analíticos tradicionais, discutidos em \citep{Avellaneda2008}, que desenvolve uma estratégia baseada em um modelo de ordem limite para lidar com o problema de risco de inventário, ou \citep{rao2020stochastic}, que implementa uma solução analítica para tempo contínuo. No entanto, essas abordagens frequentemente enfrentam desafios ao lidar com a complexidade dinâmica do ambiente financeiro \citep{Gasperov2021}.

Outra linha de pesquisa discutida em \citep{Ganesh2019} explora o uso de aprendizado por reforço para market making em um ambiente de mercado multiagente. Essa abordagem, especificamente usando o algoritmo Soft Actor-Critic (SAC), demonstrou a capacidade de aprender comportamentos adaptativos e estratégias eficazes \citep{bakshaev2020}. O aprendizado por reforço oferece a vantagem de adaptabilidade contínua em resposta às mudanças nas condições do mercado \citep{Sutton2018}.

Ao considerar a eficácia dessas abordagens, estudos como o de \citep{WOS:000747190900001} aplicam o aprendizado por reforço usando redes neurais de memória de curto prazo (LSTM) para extrair padrões temporais diretamente dos livros de ordens. Essa técnica, conhecida como Deep Reinforcement Learning Market Making, destaca a capacidade de lidar com relações complexas entre as condições de mercado e as ações estratégicas \citep{WOS:000747190900001}.

Apesar desses avanços, é importante reconhecer que diferentes abordagens apresentam vantagens e desvantagens. Métodos analíticos tradicionais podem oferecer interpretabilidade, mas podem ser limitados em sua capacidade de lidar com dinâmicas complexas e adaptação contínua. Por outro lado, algoritmos de aprendizado por reforço, como aqueles baseados em DQL, destacam-se pela adaptabilidade, mas podem exigir uma quantidade significativa de dados para treinamento e podem apresentar desafios na interpretabilidade do modelo \citep{WOS:000963297000001}. Assim, para o uso do paradigma de aprendizado por reforço foi escolhido para obter a solução da equação de otimalidade de Bellman e da política do agente.

Por fim, a contribuição da pesquisa proposta será inserir no agente uma aversão não apenas ao risco de mercado e inventário, como também ao risco noturno, inserindo-se uma restrição adicional de que ao final do dia não haja exposição ao mercado. 
Existem algumas alternativas para formalizar matematicamente essa restrição:
\begin{enumerate}
    \item De forma simplificada, no final do dia o agente não pode ter nenhum ativo em posição: 
    \begin{equation}
        I_{T} = 0
        \label{eq:inventory_restriction}
    \end{equation}
    \item Com uma abordagem mais complexa, se houver alguma posição restante, o agente precisa \textit{headgear}\footnote{De maneira simplificada, o \textit{hedge} consiste em comprar ou vender ativos que tenham uma exposição ao risco oposta aos riscos da carteira atual, de modo a equilibrar a posição.} sua exposição ao risco ao participar em outros mercados abertos no momento, abordagem que chamamos de \textit{market making} \textbf{simultâneo}.
\end{enumerate}
