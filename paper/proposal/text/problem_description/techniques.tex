O objetivo principal do agente de \textit{MM} é decidir dentro do intervalo de preços possíveis para o ativo o que proporcione um maior retorno financeiro para o menor risco associado, de acordo com a fronteira eficiente de execução ótima \citep{almgren2000}. O agente também pode decidir a quantidade $Q$ de ações ofertadas por determinado preço $p$, mas não tem controle direto sobre as quantidades efetivamente negociadas. Ou seja, a quantidade executada $N \leq Q$ é uma variável aleatória, dado que não é possível determinar de antemão quanto o agente agressor irá negociar com o agente de \textit{market-making} \citep{rao2020stochastic}.

É possível representar essa dinâmica na forma de um problema de otimização estocástica com restrições, onde o objetivo do agente (ou controle) consiste em obter uma aproximação ótima da equação de valores-estados de Bellman $Q^*(s, a) = \mathbb{E} \left[ R_{t+1} + \gamma \cdot \max_{a'} Q^*(S_{t+1}, a') \mid S_t = s, A_t = a \right]$ para um processo de decisão de Markov (ou \textit{Markov Decision Process} em inglês).

De modo a obter a solução para a equação de otimalidade de bellman, definimos um processo de decisão de Markov na seção anterior $(\mathcal{S}, \mathcal{A}, T_{a}, R_{a})$, assim como a equação de ação-estado de otimalidade de Bellman. Para sistemas dinâmicos simples, é possível obter a função de valores de ação-estado analiticamente, como proposto em \citep{Avellaneda2008} e \citep{rao2020stochastic} para espaços contínuos. Existem também métodos de aproximação numérica presentes na literatura de controle estocástico, como os métodos de programação dinâmica, que consistem em aproximar os valores para as probabilidades de transição da política (\textit{policy-iteration}) ou da função valor (\textit{value-iteration}).

Por fim, o paradigma de aprendizado por reforço possibilita o uso de algumas técnicas numéricas, que aproximam os valores de $V$ ou $Q$, ou de ambos. Dentre as técnicas consideradas estão:
\begin{itemize}
	\item \textbf{Temporal Difference Learning (\textit{TD Learning})}
	\begin{itemize}
		\item O \textit{TD Learning} é uma técnica de aprendizado por reforço que visa aprender a função valor de um estado ou ação de forma incremental com base nas diferenças temporais entre as estimativas sucessivas.
		\item A atualização típica do TD learning para um estado \(s\) é dada por \(V(s) \leftarrow V(s) + \alpha \cdot (R + \gamma \cdot V(s') - V(s))\), onde $R$ é a recompensa imediata, \(\gamma\) é o fator de desconto, \(s'\) é o próximo estado, e \(\alpha\) é a taxa de aprendizado.
	\end{itemize}
	
	\item \textbf{\textit{Q-Learning}}
	\begin{itemize}
		\item O \textit{Q-Learning} é um método de aprendizado por reforço que visa aproximar os valores da função Q, que representa a qualidade de tomar uma ação específica em um determinado estado.
		\item A regra de atualização típica do Q-valor é \(Q(s, a) \leftarrow Q(s, a) + \alpha \cdot (R + \gamma \cdot \max_{a'} Q(s', a') - Q(s, a))\), onde \(s'\) é o próximo estado, \(R\) é a recompensa, \(\gamma\) é o fator de desconto e \(\alpha\) é a taxa de aprendizado.
	\end{itemize}
	
	\item \textbf{\textit{Actor-Critic Learning}}
	
	\begin{itemize}
		\item O AC-learning envolve duas partes principais: o crítico (critic) que avalia as ações, e o ator (actor) que escolhe as ações. O objetivo é otimizar a política do ator com base nas avaliações do crítico.
		\item O crítico aprende uma função de valor como no TD learning, enquanto o ator atualiza a política para maximizar os valores de ação estimados pelo crítico.
	\end{itemize}
	
	\item \textbf{\textit{Deep Q-Learning (DQL)}}
	
	\begin{itemize}
		\item O DQL é uma extensão do Q-Learning que incorpora redes neurais para lidar com espaços de estados e ações contínuos ou de alta dimensionalidade.
		\item \textbf{Redes Neurais:} A função Q é aproximada por uma rede neural profunda. O algoritmo utiliza a experiência passada armazenada em um buffer de repetição para realizar atualizações mais estáveis.
		\item A atualização do Q-valor torna-se \(Q(s, a) \leftarrow Q(s, a) + \alpha \cdot (R + \gamma \cdot \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta))\), onde \(\theta\) são os parâmetros da rede neural, e \(\theta^-\) representa os parâmetros da rede no passo anterior.
	\end{itemize}
\end{itemize}

O problema passa a ser representado pela nova tupla $(V, \tau, \pi)$ onde $V$ e $\tau$ são observados como saída do ambiente. O paradigma de Aprendizado por Reforço permite que o agente se adapte a mudanças nas condições de mercado ao longo do tempo, e que a política escolhida considere o impacto das transações do próprio agente sobre o mercado, tornando-o mais resiliente a flutuações causadas pelas próprias ações. Uma vez treinado, o agente pode ser usado para tomar decisões em tempo real no mercado financeiro, proporcionando uma vantagem competitiva para instituições financeiras e o avanço da pesquisa em finanças quantitativas e aprendizado de máquina, abrindo novas possibilidades para o desenvolvimento de estratégias de negociação mais eficientes e robustas. A contribuição da pesquisa proposta será tornar o agente adverso ao risco noturno, inserindo-se também uma restrição adicional, de que ao final do dia não haja exposição a riscos de mercado. 
Existem algumas alternativas para formalizar matematicamente essa restrição:
\begin{enumerate}
    \item No final do dia, o agente não pode ter nenhum ativo em posição: 
    \begin{equation} \label{overnight_restriction}
        \sum_{i = 0}^{n} q_{T, i}^{a}  = \sum_{i=0}^{m} q_{T, i}^{b} = 0
    \end{equation}
    \item No final do dia, se houver alguma posição restante, o agente precisa \textit{headgear}\footnote{De maneira simplificada, o \textit{hedge} consiste em comprar ou vender ativos que tenham uma exposição ao risco oposta aos riscos da carteira atual, de modo a equilibrar a posição.} sua exposição ao risco ao participar em outros mercados abertos no momento, abordagem que chamamos de \textit{market making} \textbf{simultâneo}.
\end{enumerate}
