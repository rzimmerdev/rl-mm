O objetivo principal do agente de \textit{MM} é decidir dentro do intervalo de preços possíveis para o ativo o que proporcione um maior retorno financeiro para o menor risco associado, de acordo com a fronteira eficiente de execução ótima \citep{almgren2000}. O agente também pode decidir a quantidade $Q$ de ações ofertadas por determinado preço $p$, mas não tem controle direto sobre as quantidades efetivamente negociadas. Ou seja, a quantidade executada $N \leq Q$ é uma variável aleatória, dado que não é possível determinar de antemão quanto o agente agressor irá negociar com o agente de \textit{market-making} \citep{rao2020stochastic}.

É possível representar essa dinâmica na forma de um problema de otimização estocástica com restrições, onde o objetivo do agente (ou controle) consiste em obter uma aproximação ótima da equação de valores-estados de Bellman $Q^{*}(s, a) = \mathbb{E} \left[ R_{t+1} + \gamma \cdot \max_{a'} Q^{*}(S_{t+1}, a') \mid S_t = s, A_t = a \right]$ para um processo de decisão de Markov (ou \textit{Markov Decision Process} em inglês). Para processos com ambientes mais simples é possível obter soluções analíticas, como proposto por \citep{Avellaneda2008} e \citep{rao2020stochastic} para intervalos de tempo contínuo. Existem métodos numéricos iterativos, que consistem em aproximar os valores das probabilidades de transição da política (\textit{policy-iteration}) ou da função valor (\textit{value-iteration}) por iterações consecutivas no ambiente.

\begin{itemize}
	\item \textbf{Temporal Difference Learning}
	\begin{itemize}
		\item O \textit{temporal difference learning}(\textit{TD learning}) é uma técnica de aprendizado por reforço que visa aprender a função valor de um estado ou ação de forma incremental com base nas diferenças temporais entre as estimativas sucessivas.
		\item A atualização típica do \textit{TD learning} para um estado $s$ é dada por $(V(s) \leftarrow V(s) + \alpha \cdot (R + \gamma \cdot V(s') - V(s))$, onde $R$ é a recompensa imediata, $\gamma$ é o fator de desconto, $s'$ é o próximo estado, e $\alpha$ é a taxa de aprendizado.
	\end{itemize}
	
	\item \textbf{\textit{Q-Learning}}
	\begin{itemize}
		\item O Q-Learning é um método de aprendizado por reforço que visa aprender a função Q, que representa a qualidade de tomar uma ação específica em um determinado estado.
		\item A regra de atualização típica do Q-valor é $Q(s, a) \leftarrow Q(s, a) + \alpha \cdot (R + \gamma \cdot \max_{a'} Q(s', a') - Q(s, a))$, onde $s'$ é o próximo estado, $R$ é a recompensa, $\gamma$ é o fator de desconto e $\alpha$ é a taxa de aprendizado.
	\end{itemize}
	
	\item \textbf{\textit{Actor-Critic Learning}}
	\begin{itemize}
		\item O AC-learning envolve duas partes principais: o crítico (critic) que avalia as ações, e o ator (actor) que escolhe as ações. O objetivo é otimizar a política do ator com base nas avaliações do crítico.
		\item O crítico aprende uma função de valor como no TD learning, enquanto o ator atualiza a política para maximizar os valores de ação estimados pelo crítico.
	\end{itemize}
	
	\item \textbf{\textit{Deep Q-Learning (DQL)}}
	\begin{itemize}
		\item O \textit{DQL} é uma extensão do Q-Learning que incorpora redes neurais para lidar com espaços de estados e ações contínuos ou de alta dimensionalidade.
		\item \textbf{Redes Neurais:} A função Q é aproximada por uma rede neural profunda, onde a experiência passada é armazenada em um buffer de repetição para realizar atualizações mais estáveis.
		\item A atualização do Q-valor torna-se $Q(s, a) \leftarrow Q(s, a) + \alpha \cdot (R + \gamma \cdot \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta))$, onde $\theta$ são os parâmetros da rede neural, e $\theta^-$ representa os parâmetros da rede no passo anterior.
	\end{itemize}
\end{itemize}

O paradigma de Aprendizado por Reforço permite que o agente se adapte a mudanças nas condições de mercado ao longo do tempo, assim como seja capaz de convergir para um valor de $Q$. Uma vez treinado, o agente será testado com dados de mercado real. Por fim, a contribuição da pesquisa proposta será tornar o agente adverso ao risco noturno, inserindo-se também uma restrição adicional, de que ao final do dia não haja exposição a riscos de mercado. 
Existem algumas alternativas para formalizar matematicamente essa restrição:
\begin{enumerate}
    \item No final do dia, o agente não pode ter nenhum ativo em posição: 
    \begin{equation} \label{overnight_restriction}
        \sum_{i = 0}^{n} q_{T, i}^{a}  = \sum_{i=0}^{m} q_{T, i}^{b} = 0
    \end{equation}
    \item No final do dia, se houver alguma posição restante, o agente precisa \textit{headgear}\footnote{De maneira simplificada, o \textit{hedge} consiste em comprar ou vender ativos que tenham uma exposição ao risco oposta aos riscos da carteira atual, de modo a equilibrar a posição.} sua exposição ao risco ao participar em outros mercados abertos no momento, abordagem que chamamos de \textit{market making} \textbf{simultâneo}.
\end{enumerate}
