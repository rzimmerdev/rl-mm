\subsection{Definição formal do \textit{market making}}

Os mercados normalmente definem uma quantidade mínima por lote ofertado. Se uma bolsa determina um mínimo de 100 ações um vendedor não poderia criar uma oferta de 50 ações. Se os preços ofertados no momento permitirem a execução de um negócio, a transação ocorrerá em cima da menor quantidade entre as duas ofertas, ou seja, se uma ordem de venda de 100 ações fecha um negócio com uma ordem de compra de 200 ações por exemplo, a quantidade negociada será de apenas 100. O restante da oferta de compra, i.e. 100 ações, continua no livro de oferta de compras, enquanto no lado da venda, a próxima oferta de maior preferência vai para o topo do livro. 

Definimos como $a^{i}_{t}$ o preço de uma determinada oferta $i$ de venda com \footnote{A melhor oferta está na posição $i = 0$ da fila decrescente para vendas, e crescente para compras.} no instante $t$ e $b^{j}_{t}$ como uma oferta $j$ de compra, onde $i$ e $j$ são as posições de cada oferta em seus respectivos lados, então para tais ofertas no instante $t$, haverá uma transação se $a^{j}_{t} <= b^{i}_{t}$. Além disso, a diferença entre as ofertas $a^{0}_{t} - b^{0}_{t} = \mathbf{\Delta}_t$ é chamado de \textit{spread} no momento $t$.

O objetivo de uma estratégia de \textit{market making} (\textit{MM}) nesse contexto é criar ofertas de compra com valor maior que as já existentes, ou menor para ofertas de venda

\begin{itemize}
    \item de venda, representadas por $a^{mm}_{t} < a^{0}_{t}$; ou 
    \item de compra, representadas por $b^{mm}_{t} > b^{0}_{t}$;
\end{itemize}
ou seja, quaisquer ordens criadas por um agente de \textit{MM} não geram novas transações no instante \textit{t}.

Para ilustrar melhor o funcionamento do livro de ordens, imagine que um agente qualquer de \textit{MM} tenha a \textbf{única oferta de venda} pelo preço de $a^{mm}_{t}$ ou mais no mercado, em que $mm = 0$. Caso surja uma nova oferta de \textbf{compra} com melhor preço $b^{n}_{t} \geq b^{0}_{t}$ que também esteja acima do melhor preço de venda $b^{n}_{t} \geq a^{mm}_{t}$, isso acarretará em um negócio pelo preço $a^{0}$ no instante $t$ e a ordem com preço $b^{n}_{t}$ é chamada de ordem de mercado. Após o motor de transações da bolsa receber essa nova ordem, uma transação ocorre e ambas ofertas são removidas do livro de ordens. Em seguida, o agente tem sua posição $r_{t}$ ajustada:
\begin{equation}
    r_{t + 1}:= r_{t} + a^{mm}_{t}\cdot q,
\end{equation}
sendo $r_{t}$ o valor da posição do agente no instante $t$ e $q$ a quantidade de ativos negociadas na transação em questão.

O agente em seguida escolhe esperar ou realizar uma das ações abaixo:
\begin{enumerate}
    \item inserir uma nova oferta de venda, substituindo a oferta $a_t^{mm}$ anterior;
    \item inserir ou ajustar uma oferta de compra existente no livro de ofertas de compras;
\end{enumerate}

A decisão do agente irá depender de sua expectativa sobre a evolução do mercado, assim como do processo de chegada de ordens de outros agentes, representados pelo identificador $n$, incluindo, mas não limitado à
\begin{itemize}
    \item probabilidade de chegar uma oferta de compra com preço superior $Pr(b_{t + s}^{n} > b_{t}^{mm})$, $s > 0$; 
    \item probabilidade de chegar uma oferta de venda com preço inferior $Pr(a_{t + s}^{n} \leq a_{t}^{mm})$, $s > 0$;
    \item liquidez esperada para o mercado a partir de $t$ até o momento de fechamento do pregão $T$;
    \item risco futuro da posição ultrapassar os limites estabelecidos pelas corretoras.
\end{itemize}

De modo a simplificar as equações adiantes e utilizar uma notação mais comumente usada no contexto de ordens limite, definimos $\delta_{t}(a): \mathbb{R} \rightarrow \mathbb{R} = |a - p_{t}|$ como a diferença entre o preço $a$ e o preço de mercado $p$ da ação subjacente no momento $t$.
A função $\delta_{t}(a, q): \mathbb{R}^{2} \rightarrow \mathbb{R} = s_{t}(a) \cdot q$ mapeia o impacto de $q$ ações negociadas na carteira do agente sob o \textit{spread} parcial $\delta_t$.
O retorno $r$ obtido pelo agente no momento $t$ é a diferença do impacto de todas ordens agrupadas nos conjuntos de ofertas de venda $A = \{(a_{0}, Qa_{0}), ..., (a_{n}, Qa_{n})\}$ e de compra $B = \{(b_{0}, Qb_{0}), ..., (b_{n}, Qb_{m})\}$, onde cada elemento é o par preço $a_{i}$ e quantidade ofertada $Q_{i}$ para o título financeiro $i$.
\begin{equation} \label{return}
	\begin{aligned}
		r_{t} = \sum_{(a, Q) \in A} \delta_{t}(a, q_{a, t}) \\
		-\sum_{(b, Q) \in B} \delta_{t}(b, q_{b, t}) \\
		\forall t < T
	\end{aligned}
\end{equation}
Note que $q_{a, t}$ e $q_{b, t}$ são as quantidades efetivamente executadas da ordem, podendo ser menor (no caso de uma ordem parcial) ou igual à $Q$ (ordem total), onde $Q$ é a quantidade inicialmente ofertada pelo agente para a ação pelo preço $a$.
Para considerar custos de transação — que incluem tipicamente custos da corretora e emolumentos das bolsas — basta alterar a expressão para $r_{t} := r_{t} - c$, sendo $c$ o custo total de todas transações realizadas.

O agente de \textit{MM}, por fim, observa a sua posição acumulada durante todo o período de negociação $T$ para decidir se obteve retorno positivo ou negativo:
\begin{equation} \label{return_accumulated}
    R_{T} = \sum_{t=0}^{T} r_t
\end{equation}
Define-se matematicamente o valor da posição $R_T$ como a agregação das receitas de vendas, e dos custos de compra e de transações até o momento $T$. 

\subsection{Modelagem do agente de \textit{market making} e do objetivo}
O objetivo principal do agente de \textit{MM} é decidir dentro do intervalo de preços possíveis para uma ação o valor que proporcione o maior retorno para o menor risco associado, de acordo com a teoria da fronteira do mercado eficiente \citep{markowitz1952}. O agente também pode decidir a quantidade de ações ofertadas por determinado preço, mas não tem controle direto sobre as quantidades efetivamente negociadas. Ou seja, a quantidade executada $q$ é uma variável estocástica, tal que $P(q = Q), \ \forall q <= Q$ é a probabilidade de que uma oferta de $Q$ ações seja executada por completo em uma ordem. 

Podemos consequentemente modelar o agente como um problema de otimização estocástica com restrições (também chamado de programação estocástica), onde o objetivo inicial do agente é separado em duas etapas: maximização do \textit{bid-ask spread} $\Delta_{t}(a, b) = \delta_{t}(a) + \delta_{t}(b) = |a - b|$  para uma mesma ação; maximização da quantidade executada esperada $\mathbb{E} [q_{a, t}]$ e $\mathbb{E} [q_{b, t}]$ de ordens de venda e compra realizadas em cima do maior \textit{spread} $\Delta_{t}(a, b)$, garantindo que a negociação ocorra na fronteira eficiente. As variáveis de decisão são as combinações possíveis de ofertas de venda e compra — $A$ e $B$\footnote{\ Note que o agente não decide a quantidade executada $q$, apenas a quantidade ofertada $Q$}. A função objetivo do problema é o valor esperado do retorno diário, considerando a incerteza da quantidade executada $q_{i, t} \leq Q_{i}$ por ordem. Substituindo o valor de $q$ na equação \ref{return} pelo seu valor esperado:

\begin{equation}
	\begin{aligned}
		r_{t} = \sum_{(a, Q) \in A} \delta_{t}(a, \mathbb{E}[q_{a, t}]) \\
		-\sum_{(b, Q) \in B} \delta_{t}(b, \mathbb{E}[q_{b, t}]) \\
		\forall t < T
	\end{aligned}
\end{equation}
e o retorno diário acumulado da equação \ref{return_accumulated} é utilizado como função objetiva que se deseja maximizar:
\begin{equation}
	\begin{aligned}
		\max_{A, B} \quad & \sum_{t=0}^{T} \ \mathbb{E} [r_{t}]
	\end{aligned}
\end{equation}

De modo a obter uma solução para a otimização estocástica definimos o agente como um processo de decisão de Markov $(\mathcal{S}, \mathcal{A}, T, r)$, buscando modelar o problema para o paradigma de Aprendizado por Reforço:

\begin{description}
	\item[$\mathcal{S}$] 
	é o espaço de estados possíveis, representado pelo conjunto $\{(A_{t}, B_{t}, L_{t}) \ | \ t < T\}$, onde cada estado $s \in \mathcal{S}$ é uma combinação possível de ofertas de venda e de compra do agente, e o livro de ordens limite no momento $t$;
	\item[$\mathcal{A}$] é o espaço de ações que o agente pode realizar, ou seja, a combinação de novos \textit{spreads} $\delta(a_{i})$ e novas quantidades $Q^{+}_{i}$ (ou $Q^{-}_{i}$ caso a quantidade ofertada diminua) que atualizem os conjuntos $A$ e $B$ de ofertas de venda e compra existentes.
	\item[\textit{T}] são as transições possíveis entre estados dado uma ação tomada pelo agente. São representadas pela função de transição $T :  \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0, 1]$, que mapeia o estado atual e a ação tomada para a probabilidade de ir para um estado subjacente. No caso do agente de \textit{market-making}, a função $T$ recebe o estado atual $s$ e a ação $a$ tomada pelo agente (conjunto de \textit{spreads} e quantidades ofertadas atualizadas). Em seguida recebe um possível estado futuro $s' \in \mathcal{S}$ e retorna a probabilidade de transição $T(s, a, s') = Pr(S_{t+1} = s' \ | \ S_{t} = s, A = a)$.
	\item[\textit{r}] é a função de recompensa da cadeia aleatória, que mapeia o estado atual e a ação do agente para a probabilidade de uma recompensa ocorrer caso a transição para um determinado estado seguinte ocorra.
\end{description}

Utilizando a definição do agente como um Processo de Decisão de Markov (MDP), podemos empregar uma abordagem de Aprendizado por Reforço (RL) para simular o ambiente e encontrar uma política de decisão de preços ótimas. Esta seção explora em detalhes como o Aprendizado por Reforço é aplicado a esse contexto desafiador e por que essa abordagem é crucial.

\begin{itemize}
	\item Trajetória ($\tau$): é a sequência de estados observados e ações tomadas ao longo do tempo. No contexto de \textit{MM}, uma trajetória consiste em uma série de estados do mercado $s_{t} = (A_{t}, B_{t}, L_{t})$ seguidas da ação em cima desse estado $a_{t} = \{(\delta_{t}(p_{i}), Q_{i}) \ | \ \forall i \}$. Essas trajetórias representam a jornada do agente no mercado financeiro, incluindo suas ações e interações com o ambiente.
	
	\[
	\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, a_T)
	\]
	
	Onde \(s_t\) é o estado no tempo \(t\), e \(a_t\) é a ação tomada no tempo \(t\).
	
	\item Política ($\pi$): função que mapeia o estado atual (\textit{spreads} $\delta$ e quantidades $Q$) para a escolha de ações (ofertas de compra e venda). Através de algoritmos de otimização de decisão (\textit{Policy Optimization} e \textit{Q-Learning}), nosso objetivo é encontrar uma política ótima que permita ao agente tomar decisões que maximizem seus retornos no mercado. Essa política é fundamental para determinar como o agente se comporta em diferentes situações de mercado.
	
	\[
	\pi(s) \rightarrow a
	\]
	
	Essa função determina como o agente toma decisões em diferentes estados.
	
	\item Função de Valor ($V$): estima o valor esperado acumulado que o agente pode obter ao seguir a política \(\pi\) a partir de um estado inicial. No contexto do agente de market making, \(V\) depende do preço de venda \(a\) e da quantidade \(Q\), bem como da política do agente. Através do Aprendizado por Reforço, podemos calcular \(V\) para avaliar quão bom é um estado, o que orienta o agente na seleção de ações que maximizam seu desempenho global.
	
	\begin{equation*}
		V(s_0) = \mathbb{E}\left[\sum_{t=0}^{T} \gamma^t \cdot r(s_t, \pi(s_t)) \right]
		\end{equation*}
\end{itemize}


**Por que fazer isso?**

A aplicação do Aprendizado por Reforço nesse cenário é de suma importância por diversas razões:

- **Tomada de Decisão Ótima:** Através da aprendizagem de uma política de decisão ótima, o agente pode aprimorar suas estratégias de negociação, maximizando seus lucros em um ambiente financeiro complexo e estocástico.

- **Adaptação a Mudanças:** O Aprendizado por Reforço permite que o agente se adapte a mudanças nas condições de mercado ao longo do tempo, tornando-o mais resiliente a flutuações e incertezas.

- **Eficiência e Aplicação em Tempo Real:** Uma vez treinado, o agente pode ser usado para tomar decisões em tempo real no mercado financeiro, proporcionando uma vantagem competitiva para instituições financeiras e traders.

- **Contribuição à Pesquisa:** A modelagem de um agente de market making como um MDP e a aplicação de RL contribuem para o avanço da pesquisa em finanças quantitativas e aprendizado de máquina, abrindo novas possibilidades para o desenvolvimento de estratégias de negociação mais eficientes e robustas.

Essa abordagem combina os princípios do Aprendizado por Reforço com o complexo cenário do mercado financeiro, permitindo ao agente aprimorar suas habilidades de negociação e, ao mesmo tempo, contribuir para o progresso da pesquisa em finanças quantitativas.

De modo a tornar o agente adverso ao risco noturno, insere-se também uma restrição adicional, de que ao final do dia não haja exposição a riscos de mercado. 
Existem algumas alternativas para formalizar matematicamente essa restrição:
\begin{enumerate}
    \item No final do dia, o agente não pode ter nenhum ativo em posição: 
    \begin{equation}
        \sum_{b=1}^{B_t} q_b  - \sum_{a=1}^{A_t} q_a = 0\label{eq:eod_restriction}
    \end{equation}
    \item No final do dia, se houver alguma posição, o agente precisa \textit{headgear} a exposição ao risco, comprando (vendendo) futuros ou outros derivativos, dependendo da posição remanescente.\footnote{De maneira simplificada, o \textit{hedge} consiste em comprar ou vender ativos que tenham uma exposição ao risco oposta aos riscos da carteira atual, de modo a equilibrar a posição.}
\end{enumerate}

\subsection{\textit{Market making} simultâneo}
Ao considerarmos a situação em que o MM aplica a sua estratégia em diversos mercados simultaneamente observamos um aumento da complexidade, mas também das alternativas para lidar com riscos envolvidos - chamamos essa situação de MM \textbf{simultâneo} ou \textbf{multivariado}.

Para tal, a posição $V_{T}$ do agente passa a ser a soma de todas posições parciais $V_{T, k}$, onde $k$ é uma bolsa onde o agente possui ações:
\begin{eqnarray*}
    V_t &=& \sum_{k=1}^N V_{T, k}\\
\end{eqnarray*}

O objetivo principal (\ref{eq:target_fct}) continua o mesmo, e mantém-se a restrição (\ref{eq:eod_restriction}). Contudo, surgem novas alternativas para proteção da carteira durante a noite:

\begin{enumerate}
    \item O agente pode avaliar o risco global da carteira, e incluir um único ativo de proteção contra o risco global ao final do dia.
    \item Se o ativo for negociado em múltiplas bolsas (chamado também de ativo \textit{co-listed}), o agente pode continuar a negociação deste em outra bolsa caso uma delas esteja fechada.
\end{enumerate}

Considerando um cenário em que haja ações em \textit{co-listing}, surge a possibilidade de criação de estratégias mais sofisticadas, permitindo a implementação dos itens mencionados acima.
 