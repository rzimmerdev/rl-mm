\subsection{Market Making}
O market-making é uma forma de negociação (\textit{trading} em inglês) que de forma simplificada consiste em comprar certo ativo a um determinado preço e vender-lo a um preço maior.
Como os preços de compra e venda dos ativos são afetados pela demanda e oferta no momento, cada agente no mercado exerce certa influência sobre o valor de um ativo, a depender das ofertas que o mesmo mantém para tal ativo no livro de ordens limite (\textit{limit order book}, ou \textit{LOB} em inglês). 

No \textit{LOB} os registros se dão primeiramente por ordem de preço, e em segundo por data de criação: as ofertas com melhor preço (tanto para compra como para venda) ficam no topo do livro, e caso tenham o mesmo valor entre si tem sua posição desempatada pela ordem temporal de chegada. No livro de compras, a melhor oferta é a que oferece o maior preço, e o contrário vale para o livro de vendas.

Nas bolsas de valores digitais a execução de uma transação é automática e auxiliada por um sistema chamado de \textit{matching engine} - ou seja, um motor para pareamento de ordens. Esse sistema verifica se para a melhor oferta de compra (ou venda) existe outra correspondente no livro de venda (ou compra) com valor menor ou igual (ou maior ou igual para venda). Após o pareamento, a bolsa anuncia o a execução da ordem e as ofertas relacionadas são removidas dos livros.

Em suma, os principais elementos do market making incluem, mas não se limitam à:

\begin{itemize}
	\item Spread: É a diferença entre o preço de compra (\textit{bid} em inglês) e o preço de venda (\textit{ask} em inglês) entre duas ofertas. O market maker busca obter os melhores valores para esses preços, e eventualmente lucrar com a diferença entre eles.
	
	\item Livro de Ordens Limite: É um conjunto ordenado onde as ofertas de compra e venda são registradas. Também é permitido o ajuste dos preços de compra e venda de ofertas existentes por parte dos agentes.
	
	\item Gestão de Risco: Todos \textit{market-makers} enfrentam riscos em suas negociações, entre eles o risco de inventário e risco de mercado. O risco de inventário ocorre quando o market maker mantém uma posição desequilibrada entre ativos comprados e vendidos, enquanto o risco de mercado está relacionado às flutuações nos preços dos ativos.
\end{itemize}

No contexto deste projeto, o foco da pesquisa será a otimização de uma estratégia de \textit{market-making} que minimize o risco de inventário durante a noite. A estratégia será composta por um agente responsável pela interação com o mercado e alocação de preços sob políticas para redução de risco. Mais adiante, será apresentado a modelagem do problema como uma cadeia de decisão de Markov. Tendo definido o espaço de ações possíveis para o agente e de observações possíveis do mercado, modelaremos a equação de otimalidade de Bellman para os estados do mercado, e discutiremos os algoritmos de Aprendizado por Reforço (RL) que foram escolhidos como abordagem para obter um agente ótimo capaz de aproximar numericamente os preços ótimos.

\subsection{Sistemas dinâmicos e Aprendizado por Reforço}
O Aprendizado por Reforço (RL) é um paradigma de aprendizado de máquina baseado em princípios da psicologia comportamental e em otimização estocástica, especificamente tarefas de controle e sistemas dinâmicos. Simplificadamente, trata-se de uma técnica para modelagem, simulação e treino de um agente. Tal agente é capaz de interagir com um sistema, modelado a partir de um processo de decisão de Markov, onde o objetivo do agente é obter uma política de interação com o ambiente que maximize sua recompensa cumulativa ao longo do tempo, chamada de "retorno" (note que no contexto de \textit{RL}, o retorno é diferente do retorno financeiro em si).

A definição de uma cadeia de decisão com a qual o agente interagirá é uma tupla $MDP = (\mathcal{S}, \mathcal{A}, T_{a}, R_{a})$, onde os elementos que a compõem são:

\begin{description}
	\item[$\mathcal{S}$] 
	é o espaço de estados possíveis, representado por um conjunto de estados, onde cada estado $s \in \mathcal{S}$ é uma tupla de $n$ valores numéricos observáveis $s = (x_{0}, x_{1}, \ldots, x_{n})$. Cada estado é uma observação possível da dinâmica do sistema em determinado momento;
	
	\item[$\mathcal{A}$] é o espaço de ações que o agente pode realizar. Uma ação $a \in \mathcal{A}$ é uma tupla com $m$ valores numéricos tal que $a = (y_{0}, y_{1}, \ldots, y_{m})$ são os valores que o agente aplica no sistema, e transiciona à um novo estado, recebendo uma recompensa.
	
	\item[\textit{T}] é o operador de transição, que representa as transições possíveis entre estado atual $s$ e possível estado futuro $s'$, dado uma ação $a$ tomada pelo agente, tal qual \(T : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0, 1]\). A função $T$ fornece uma função de densidade de probabilidade, tal que $T(s, a, s') = f(s' \,|\, s, a)$ onde \(f(s' \,|\, s, a) \in \mathcal{L}([0, 1])\) e $\mathcal{L}$ é o espaço de probabilidade padrão de Lebesgue.
	
	\item[\textit{R}] é a função de recompensa da cadeia de markov, que mapeia uma determinada transição à probabilidade de uma recompensa ocorrer 
	$R : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$, onde $R(s, a, s') = \int_{\mathbb{R}} r \cdot f(r \,|\, s, a, s') \, dr$ e $f$ é a função de densidade condicional à transição ocorrida, e a integral retorna o valor esperado da recompensa.
\end{description}

Considerando o contexto de controle estocástico, um agente ou controle é efetivamente a política de escolha de ações. Dado um estado atual $s$ e uma ação $a$, a política dá a probabilidade do agente executar e enviar à cadeia tal ação. Uma política estocástica $\pi$ qualquer é representada por:

\begin{equation}
	\begin{aligned}
	\pi(a \,|\, s) = g(a \,|\, s)\text{, onde }\\ g(a \,|\, s) \in \mathcal{L}([0, 1])
	\end{aligned}
\end{equation}

Por fim, o objetivo do agente é obter uma política ótima que maximize o retorno obtido, que é a soma amortizada das recompensas futuras. O retorno do agente é, dado um determinado momento em um intervalo de tempo discreto $t \leq T, t \in \mathbb{Z}$, onde T é o final do período de observação, pela seguinte expressão:

\begin{equation}
	\begin{aligned}
		G_{t} = \sum_{k=0}^{T} \gamma^t \cdot R_{t + k + 1}
	\end{aligned}
\end{equation}

onde $\gamma$ é o fator de amortecimento e $R_t$ é a recompensa que o agente recebe no momento $t$, de acordo com uma função de recompensa $R$, ou seja $R_t = R(s_{t}, a_{t}, s_{t + 1})$.

A equação de maximização da recompensa que será tratada na seção a seguinte, e que forma a base para os problemas de aprendizado por reforço é a equação ação-valor de otimalidade de Bellman:


\begin{equation}
	\begin{aligned}
		Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_t \mid s_{t} = s, a_{t} = a \right]\\
		Q^{*}(s, a) = \max_{\pi} Q^{\pi}(s, a)		
	\end{aligned}
\end{equation}

\begin{itemize}
\item Política (ou \textit{Policy}, também chamado de "Controle"): Define o processo de tomada de decisões do agente, ou seja, como ele escolhe ações em resposta às observações do ambiente. Pode ser uma estratégia determinística ou estocástica.

\item Recompensa (ou \textit{Reward}): É uma medida numérica que permite ao agente interpretar quão boa ou ruim foi uma ação específica em função do estado atual do==== ambiente, assim como atribuir valores numéricos à qualidade dos estados observados ao longo do tempo. O objetivo do agente é então maximizar a recompensa cumulativa ao longo do tempo ao tomar ações que levem para estados com valores altos.

\item Ambiente (ou \textit{Environment}): Representa e simula o estado do ambiente real, assim como as possíveis ações que o agente possa realizar. É uma descrição quantitativa de elementos do ambiente e como as ações tomadas pela política resultam em mudanças no ambiente.
\end{itemize}

Técnicas de \textit{AR} podem ser aplicadas à diversos domínios, da robótica, jogos à finanças e no contexto deste projeto será o paradigma central na idealização e modelagem da estratégia de \textit{market-making}. Na próxima seção será realizada uma descrição inicial do ambiente e da modelagem do sistema à ser realizada.
