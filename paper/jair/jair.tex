%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{jair}.
\documentclass[manuscript]{jair}


\setcopyright{cc}
\copyrightyear{2025}
\acmDOI{xx.xxxx/jair.1.xxxxx}

\JAIRAE{}
\JAIRTrack{} % Insert JAIR Track Name only if part of a special track
\acmVolume{0}
\acmArticle{0}
\acmMonth{0}
\acmYear{2025}

\RequirePackage[
    datamodel=acmdatamodel,
    style=acmauthoryear,
    backend=biber,
    giveninits=true
]{biblatex}

\renewcommand*{\bibopenbracket}{(}
\renewcommand*{\bibclosebracket}{)}

%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
\addbibresource{references.bib}  % bibliography file.

\include{packages}

\newcommand{\Title}{Reinforcement Learning-Based Market Making as a Stochastic Control on Non-Stationary Limit Order Book Dynamics}

\begin{document}
    \input{cover}

    \begin{abstract}
        Reinforcement Learning has emerged as a promising framework for developing adaptive and data-driven strategies,
        enabling market makers to optimize decision-making policies based on interactions with the limit order book environment.
        This paper explores the integration of a reinforcement learning agent in a market-making context,
        where the underlying market dynamics have been explicitly modeled to capture observed stylized facts of real markets,
        including clustered order arrival times, non-stationary spreads and return drifts, stochastic order quantities and price volatility.
        These mechanisms aim to enhance stability of the resulting control agent,
        and serve to incorporate domain-specific knowledge into the agent policy learning process.
        Our contributions include a practical implementation of a market making agent based on the Proximal-Policy Optimization (PPO) algorithm,
        alongside a comparative evaluation of the agent's performance under varying market conditions via a simulator-based environment.
        As evidenced by our analysis of the financial return and risk metrics when compared to a closed-form optimal solution,
        our results suggest that the reinforcement learning agent can effectively be used under non-stationary market conditions,
        and that the proposed simulator-based environment can serve as a valuable tool for training and
        pre-training reinforcement learning agents in market-making scenarios.
    \end{abstract}

%% To be updated by authors.
%\received{20 February 2007}
%\received[accepted]{5 June 2009}

    \maketitle

    \input{textual/introduction}

    \input{textual/bibliography}

    \input{textual/methodology}

    \input{textual/model}

    \input{textual/results}

    \input{textual/conclusion}

    \printbibliography

\end{document}
\endinput
