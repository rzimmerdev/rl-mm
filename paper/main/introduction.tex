\section{Introduction}
\label{sec:introduction}

Market making in financial markets involves continuously quoting buy and sell prices, and Dealing with supply and demand imbalances,
as well as the risks of holding inventory and speculation is an intrinsic aspect of market making and a focus of recent research~\cite{Cartea2015, Gasperov2021}.
Market makers are essential, as they help narrowing bid-ask spreads, reduce price volatility, and maintain market stability,
particularly in times of uncertainty by trading and providing liquidity in the market~\cite{Glosten1985, OHara1995}.
Correctly reducing inventory risk benefits not only individual market makers but all market participants as a whole.
With the increase of computational power and data-driven automated systems, placing optimal bid and ask prices as a market maker is becoming an almost completely automatized task,
but such a transition comes with additional caveats, such as slippage, overnight inventory and adverse market movements risks on agent inventory~\cite{Cartea2015, Avellaneda2008}.

Recent research focusing on Reinforcement Learning (RL) shows promising results for optimizing placed bid-ask prices by market making strategies.
The RL framework defines market makers as agents that learn by interacting with an environment - which in our case is the limit order book -
and optimizing their decision-making policies through trial and error and the respective outcome
(also called reward \footnote{Not to be confused with financial returns or rewards.}) of the environment.
The primary objective of RL-based market making is to find the so called optimal policy that maximizes cumulative rewards,
which translates to choosing prices that provide positive spread differences while minimizing risk factors such as agent inventory or price volatility.
The RL approach is based on the Bellman equation for state values or state-action pair values,
which recursively define the value of a policy by considering the expected return of discounted future rewards.
A common implementation of the RL approach is to continuously update the agent's policy based on the sampled environment information and rewards,
allowing the agents to dynamically change their price choosing strategies and adapt to changing market conditions in real-time~\cite{Sutton2018}.

Unrestricted RL-based market making, however, can lead to potentially risky behaviors.
In financial markets, and in long high-frequency intervals of trading, inventory risk and market impact are not to be ignored,
as agents accumulating large inventory positions become exposed to significant price fluctuations and adverse market direction changes.
This makes holding inventory result in an unwanted market exposition to the market making agent, becoming thus subject to undesired risks.
Recent literature discusses the means of implementing restrictions in the RL framework to ensure stability and applicability towards real-world implementations.

In the context of this paper we will discuss restrictions in two possible forms:
as hard equality or interval limits, or as incentives integrated into the agent's reward structure.
Hard limits enforce strict numerical conditions, such as having zero inventory at market close, that must be met.
On the other hand, incentive-based restrictions simply increase the loss on rewards the closer the agent is to the restriction,
such as a negative reward the larger the inventory on market close, thus making the agent adverse to so-called overnight risk,
which will be discussed in further sections.
Such restrictions are aimed to reduce target volatility and increase the possibilities of introducing domain knowledge
and known stylized facts of the market into the learning process of the agent~\cite{Jerome2022, Selser2021}.

Our main contributions aims to be a pragmatic implementation of restrictions for market making within an RL framework,
contributing with a replicable way of minimizing undesired risk taken by the agent while maintaining the computational feasibility of the chosen architecture.
Finally, the impact on the financial returns of the RL agents averse to overnight risk will be
benchmarked against a closed-expression optimal solution under a simplified market model,
aiming to validate the usage of hand-crafted restrictions in real-world implementations.