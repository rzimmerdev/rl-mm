\section{Realized Experiments and Result Discussion}
\label{sec:realized-experiments-and-result-discussion}

\subsection{Experiment Setup}
\label{subsec:experiment-setup}

The experiments were conducted using the OpenAI Gym environment for the limit order book, which was implemented in Python.
The environment was set up with a limit order book with dynamics according to the equations described in
~\hyperref[subsec:market-model-description-and-environment-dynamics]{Section~\ref*{subsec:market-model-description-and-environment-dynamics}}.

Both restricted agents were trained using the Proximal Policy Optimization (PPO) algorithm, which is a model-free, on-policy algorithm that optimizes the policy directly.
A total of 10,000 episodes were simulated, with each episode consisting of a total time of at least 390 minutes (or 6.5 hours), with about 1 time step per minute.
Per gathered trajectory, 500 epochs were used for updating both the policy and value networks.
We used the Adam optimizer with a learning rate of $10^{-4}$ and a discount factor of 0.99.
For comparison metrics, we used the Sharpe ratio, the average daily return, and the average daily volatility.

\subsection{Experiment Results}

The agent's policy was trained using the PPO algorithm with no restrictions on the agent's actions.
