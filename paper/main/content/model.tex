\section{Implementation and Model Description}
\label{sec:implementation-and-model-description}

\subsection{Model Architecture}
\label{subsec:model-architecture}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/policy}
    \caption{Actor-Critic Model Architecture}
    \label{fig:model-architecture}
\end{figure}\

\subsection{Main Training Loop}
\label{subsec:main-training-loop}

\begin{algorithm}[H]
    \begin{algorithmic}[1]
        \Require Environment, Actor-Critic model, optimizer, number of episodes $num\_episodes$
        \State \textbf{Initialize} the environment, Actor-Critic model, and optimizer
        \For{each episode in range $num\_episodes$}
            \State \textbf{Collect trajectories:}
            \State \hspace{1em} Initialize the state: $\text{s} = \text{env.reset()}$
            \State \hspace{1em} Initialize an empty trajectory buffer
            \For{each timestep in the episode}
                \State \hspace{1em} Select action $a \sim \pi_{\theta}(s)$
                \State \hspace{1em} Observe reward $R_t$ and next state $s'$
                \State \hspace{1em} Store transition $(s, a, r)$ in the trajectory buffer
                \State \hspace{1em} Set $s \leftarrow s'$
                \If{environment done}
                    \State \hspace{1em} \textbf{end} episode
                \EndIf
            \EndFor
            \State \textbf{Compute GAE and Returns:}
            \State \hspace{1em} Compute advantages and returns using GAE
            \State \textbf{Update actor and critic according to \hyperref[alg:algorithm]{Algorithm~\ref{alg:ppo}}}
        \EndFor
    \end{algorithmic}
    \caption{Training Loop}
    \label{alg:algorithm}
\end{algorithm}

%\begin{algorithmic}
%
%\end{algorithmic}

