\section{Implementation and Model Description}
\label{sec:implementation-and-model-description}

\subsection{Model Architecture}
\label{subsec:model-architecture}

\subsection{Trajectory Gathering and Generalized Advantage Estimation}
\label{subsec:trajectory-gathering-and-gae}

\subsection{Main Training Loop}
\begin{algorithmic}
    \State \textbf{Initialize} the environment, Actor-Critic model, and optimizer
    \For{each episode in range $num\_episodes$}
        \State \textbf{Collect trajectories:}
        \State \hspace{1em} Initialize the state: $state$
        \State \hspace{1em} Initialize an empty trajectory buffer
        \For{each timestep in the episode}
            \State \hspace{1em} Select action $a$ using the Actor network
            \State \hspace{1em} Execute action $a$ and observe reward $r$ and next state $state'$
            \State \hspace{1em} Store transition $(state, a, r, state')$ in the trajectory buffer
            \State \hspace{1em} Set $state = state'$
            \If{episode ends or maximum timestep reached}
                \State \hspace{1em} Break out of timestep loop
            \EndIf
        \EndFor
        \State \textbf{Compute GAE and Returns:}
        \State \hspace{1em} Compute advantages and returns using GAE
        \State \textbf{Update model:}
        \State \hspace{1em} Update the Actor and Critic networks using the collected trajectories
        \State \hspace{1em} Compute episode reward and store it in reward history
    \EndFor
\end{algorithmic}

