\section{Introduction}
\label{sec:introduction}

Market making in financial markets involves continuously quoting buy and sell prices, and dealing with supply and demand imbalances,
and the associated risks of holding inventory and speculation are an intrinsic aspect of market making and focus of recent research~\cite{Cartea2015, Gasperov2021}.
Market makers are essential, as they help narrow bid-ask spreads, reduce price volatility, and maintain market stability,
particularly in times of uncertainty by providing liquidity in the market~\cite{Glosten1985, OHara1995}.
Correctly reducing inventory risk benefits not only individual market makers but all market participants as a whole,
due to the reduced spread required for an agent to obtain positive return.
With the increase of computational power in data-driven trading systems,
placing optimal bid and ask prices as a market maker is becoming an almost completely automatized task,
but such a transition comes with additional caveats, such as $\text{negative, not-advantageous? or what}$ slippage,
overnight inventory~\cite{Cartea2015, Avellaneda2008} and adverse market regimes~\cite{Cont2010, Bouchaud2018} and non-stationary conditions~\cite{Gasperov2021}.

Recent research focusing on Reinforcement Learning (RL) shows promising results for optimizing placed bid-ask prices by market making strategies.
Through the RL framework market makers can be defined as agents that learn by interacting with an environment,
and adapt their decision-making policies through trial and error and a numerical outcome
(also called the reward \footnote{Not to be confused with financial returns or rewards.}).
The primary objective of RL-based market making is to find the so-called optimal policy that maximizes cumulative rewards,
which translates to choosing prices that incur in transactions with positive spreads while minimizing risk factors such as agent inventory or price volatility.
The RL approach is based on the Bellman equation for state values or state-action pair values,
which recursively define the state-value of a policy as the expected return of discounted future rewards.
A common implementation of the RL approach is to continuously update the agent's policy based on the sampled environment information (also called the state) and episodic rewards,
allowing the agents to dynamically change their price-choosing strategies according to changing market observations~\cite{Sutton2018}.


The usual approach to RL-based market making is to use model-free algorithms, such as Proximal Policy Optimization (PPO) or Deep Q-Learning (DQN),
and either use historical observed market messages to reconstruct the limit order book (LOB) environment,
and thus disconsider market impact~\cite{Frey2023, Ganesh2019}, or use a stochastic model to simulate the LOB environment~\cite{Gasperov2021, Sun2022}.
The former approach is computationally expensive and requires a large amount of data to train the agent,
but usually results in more robust and generalizable policies, while the latter approach is computationally cheaper and faster to train,
but might not generalize well to unseen market conditions.

In the context of this paper, we will focus on the implementation of a RL-based market making agent and
analyze the impact of a simulator with configurable independent market dynamic processes on the agent's performance.
This approach aims to provide a more realistic and flexible environment for the agent to learn from,
and test the robustness and generalization capabilities of the RL paradigm under non-stationary
market dynamics, adverse market conditions and considering market impact.

Our main contribution is a pragmatic implementation of a simulator-based RL agent in the context of market making,
and a comparative analysis of the agent's performance under changing market conditions during the
trading day, and the impact of market impact on the agent's decision-making policies.
Finally, the impact on the financial returns of the RL agents under different market conditions will be benchmarked against a closed-expression optimal solution under a simplified market model,
aiming to validate the usage of RL-based market making agents in markets with more complex regimes and non-stationary conditions.

Unrestricted RL-based market making, however, can result in strategies with potentially high risk behavior.
In financial markets with high-frequency data ticks long intervals of trading inactivity, inventory risk and market impact are not to be ignored,
as agents accumulating large inventory positions become exposed to significant price fluctuations and adverse market direction changes, especially overnight after closing hours.
Holding large inventories therefore result in unwanted market exposition for the market making agent, and thus, undesired risk.
% Recent literature discusses the means of implementing restrictions in the RL framework to ensure stability and applicability towards real-world implementations.
% what literature? no need for text, just the reference as to why unrestriction leads to risky behavior.

%In the context of this paper we will discuss restrictions in two possible forms:
%as hard equality or interval limits, or as incentives integrated into the agent's reward structure.
%Hard limits enforce strict numerical conditions, such as having zero inventory at market close, that must be met.
%On the other hand, incentive-based restrictions simply increase the loss on rewards the closer the agent is to the restriction,
%such as a negative reward the larger the inventory on market close, thus making the agent adverse to so-called overnight risk,
%which will be discussed in further sections.
%Such restrictions are aimed to reduce target volatility and increase the possibilities of introducing domain knowledge
%and known stylized facts of the market into the learning process of the agent~\cite{Jerome2022, Selser2021}.
%
%Our main contribution is a pragmatic implementation and analysis of restriction-based RL agents in the context of market making.
%% A replicable way of minimizing undesired risk taken by the agent while maintaining the computational feasibility of the chosen architecture.
%Finally, the impact on the financial returns of the RL agents averse to overnight risk will be
%benchmarked against a closed-expression optimal solution under a simplified market model,
%aiming to validate the usage of hand-crafted restrictions in real-world implementations.
