\title{A Comparative Study of Constraint-Driven Reinforcement Learning Towards Managing Risk in Market Making}
\author{Zimmer, R. and Costa, O.}
\date{\today}

\maketitle

\begin{abstract}
    Reinforcement Learning has emerged as a promising framework for developing adaptive and data-driven strategies,
    enabling market makers to optimize decision-making policies based on interactions with the limit order book environment.
    This paper explores the integration of risk management constraints into reinforcement learning-based market making to address challenges
    such as inventory risk, adverse market movements, and overnight exposure.
    We explore and compare two types of restrictions: 
    hard limits, which enforce strict conditions like zero inventory at market close,
    and incentive-based restrictions, which penalize risky behaviors via penalties in the reward structure.
    These mechanisms aim to enhance stability and incorporate domain-specific knowledge into the RL framework.

    Our contributions include a practical implementation of a restricted agent based on the Proximal-Policy Optimization (PPO) algorithm,
    alongside a comparative evaluation of the aforementioned restriction types.
    An unrestricted agent under a simplified market model is used as a final benchmark,
    and experimental results, conducted under synthetic-realistic market conditions, are discussed.
    The incorporated restrictions are shown to demonstrate effectiveness in balancing profitability and risk management,
    as evidenced by an analysis of the financial return analysis and risk metrics.
    These findings allow for a path towards more robust and replicable approaches to the topic of dynamic risk management in market making.
\end{abstract}
